{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"facial_expression_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15154d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('facial_expression_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, size):\n",
    "    width = int(img.shape[1] * size)\n",
    "    height = int(img.shape[0] * size)\n",
    "    dimension = (width, height)\n",
    "    return cv2.resize(img, dimension, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find face encodings for recognition\n",
    "def findEncoding(images):\n",
    "    imgEncodings = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encodeimg = face_recognition.face_encodings(img)[0]  # Corrected the function name\n",
    "        imgEncodings.append(encodeimg)\n",
    "    return imgEncodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition as face_rec\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import face_recognition\n",
    "\n",
    "# Load face cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load student images and names for recognition\n",
    "path = 'student_images'\n",
    "studentImg = []\n",
    "studentName = []\n",
    "myList = os.listdir(path)\n",
    "for cl in myList:\n",
    "    curimg = cv2.imread(f'{path}/{cl}')\n",
    "    studentImg.append(curimg)\n",
    "    studentName.append(os.path.splitext(cl)[0])\n",
    "\n",
    "# Function to find face encodings for recognition\n",
    "def findEncoding(images):\n",
    "    imgEncodings = []\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encodeimg = face_recognition.face_encodings(img)[0]\n",
    "        imgEncodings.append(encodeimg)\n",
    "    return imgEncodings\n",
    "\n",
    "# Get face encodings for student images\n",
    "EncodeList = findEncoding(studentImg)\n",
    "\n",
    "# Load emotion detection model\n",
    "model = tf.keras.models.load_model('facial_expression_model.h5')  # Load your emotion detection model here\n",
    "\n",
    "# Define emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Open CSV file for writing and write headers\n",
    "with open('students_data.csv', mode='a', newline='') as file:  # Append mode\n",
    "    writer = csv.writer(file)\n",
    "    # Check if the file is empty\n",
    "    if file.tell() == 0:\n",
    "        writer.writerow(['Name', 'Expression', 'Date', 'Time'])  # Write headers\n",
    "\n",
    "    # Start video capture\n",
    "    vid = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        success, frame = vid.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Get current date and time\n",
    "        current_date = datetime.datetime.now().date()\n",
    "        current_time = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "\n",
    "        # Convert frame to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract face ROI\n",
    "            face_roi = gray[y:y + h, x:x + w]\n",
    "\n",
    "            # Resize to match model input size for emotion detection\n",
    "            face_roi_resized = cv2.resize(face_roi, (48, 48))\n",
    "            face_roi_resized = np.expand_dims(face_roi_resized, axis=0)\n",
    "            face_roi_resized = np.expand_dims(face_roi_resized, axis=-1)\n",
    "\n",
    "            # Predict emotion\n",
    "            predicted_emotion = model.predict(face_roi_resized)\n",
    "            emotion_label = emotion_labels[np.argmax(predicted_emotion)]\n",
    "\n",
    "            # Recognize student and add to CSV\n",
    "            face_gray = gray[y:y+h, x:x+w]\n",
    "            face_rgb = frame[y:y+h, x:x+w]\n",
    "            Smaller_frame = cv2.resize(face_rgb, (0, 0), None, 0.25, 0.25)\n",
    "\n",
    "            facesInFrame = face_rec.face_locations(Smaller_frame)\n",
    "            encodeFacesInFrame = face_rec.face_encodings(Smaller_frame, facesInFrame)\n",
    "\n",
    "            for encodeFace, faceLoc in zip(encodeFacesInFrame, facesInFrame):\n",
    "                matches = face_rec.compare_faces(EncodeList, encodeFace)\n",
    "                faceDis = face_rec.face_distance(EncodeList, encodeFace)\n",
    "                print(faceDis)\n",
    "                matchIndex = np.argmin(faceDis)\n",
    "\n",
    "                if matches[matchIndex]:\n",
    "                    name = studentName[matchIndex].upper()\n",
    "                    writer.writerow([name, emotion_label, current_date, current_time])\n",
    "\n",
    "                    # Draw rectangle around the face and label the emotion and name\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "                    cv2.putText(frame, f'{name}: {emotion_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Facial Expression and Person Identification', frame)\n",
    "\n",
    "        # Exit if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Flush changes to the CSV file to ensure auto-saving\n",
    "        file.flush()\n",
    "\n",
    "    # Release the video capture\n",
    "    vid.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0ffbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
